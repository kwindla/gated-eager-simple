#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""spec-examples - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Deepgram (Speech-to-Text)
- Anthropic (LLM)
- Google (LLM)
- Cartesia (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    Frame,
    InterimTranscriptionFrame,
    InterruptionFrame,
    LLMRunFrame,
    SystemFrame,
    TextFrame,
    TranscriptionFrame,
    UserStartedSpeakingFrame,
    UserStoppedSpeakingFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.anthropic.llm import AnthropicLLMService
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


class _CustomUserStartedSpeakingFrame(SystemFrame):
    pass


class _CustomUserStoppedSpeakingFrame(SystemFrame):
    pass


# Interim transcription frames are rarely accurate enough to be useful
# Whenever we get a transcription frame we should run inference
# We should close the output gate when we see UserStartedSpeakingFrame
# We should open the output gate when we see UserStoppedSpeakingFrame
# Set the aggregator timeout to 0.01
# What all this does is reduce the impact of the false-negative smart turn results


class TxFrameHandler(FrameProcessor):
    def __init__(self, gate):
        super().__init__()
        self._gate = gate

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        # Log all text frames (TranscriptionFrame and InterimTranscriptionFrame) for visibility
        if isinstance(frame, TextFrame):
            logger.info(f"{frame.name} - {frame.text}")

        # Track user speaking state
        if isinstance(frame, UserStartedSpeakingFrame):
            await self._gate.close()
            return
        elif isinstance(frame, UserStoppedSpeakingFrame):
            await self._gate.open()
            return

        if isinstance(frame, TranscriptionFrame) and direction == FrameDirection.DOWNSTREAM:
            await self.push_frame(InterruptionFrame())
            await self.push_frame(UserStartedSpeakingFrame())
            await self.push_frame(frame)
            await self.push_frame(UserStoppedSpeakingFrame())
            return

        await self.push_frame(frame, direction)


class ClearingOutputBuffer(FrameProcessor):
    """Buffers frames when user is speaking, clears buffer when user stops speaking."""

    def __init__(self):
        super().__init__()
        self._buffer = []
        self._buffering = False

    async def close(self):
        logger.info("closing gate")
        self._buffering = True
        if self._buffer:
            self._buffer.clear()

    async def open(self):
        logger.info("opening gate")
        self._buffering = False
        if self._buffer:
            for frame, direction in self._buffer:
                await self.push_frame(frame, direction)
            self._buffer.clear()

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        # Always let system frames pass through
        if isinstance(frame, SystemFrame):
            await self.push_frame(frame, direction)
            return

        # Buffer or pass through based on user speaking state
        if self._buffering and direction == FrameDirection.DOWNSTREAM:
            self._buffer.append((frame, direction))
        else:
            await self.push_frame(frame, direction)


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    # Speech-to-Text service
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    # Text-to-Speech service
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id=os.getenv(
            "CARTESIA_VOICE_ID",
            "71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        ),
    )

    # LLM service -- testing with both Claude and Gemini
    a_llm = AnthropicLLMService(
        api_key=os.getenv("ANTHROPIC_API_KEY"),
        model=os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-5-20250929"),
    )
    g_llm = GoogleLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        model=os.getenv("GOOGLE_MODEL", "gemini-2.5-flash"),
    )
    llm = a_llm

    # Video service (avatar)

    messages = [
        {
            "role": "system",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational. Use only plain text formatting. Your output will be converted to speech.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor()

    output_buffer = ClearingOutputBuffer()
    tx_frames = TxFrameHandler(gate=output_buffer)

    # Pipeline - assembled from reusable components
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            tx_frames,
            context_aggregator.user(),
            llm,
            tts,
            output_buffer,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()
